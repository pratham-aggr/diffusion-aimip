apiVersion: batch/v1
kind: Job
metadata:
  name: ${JOB_NAME}
  namespace: climate-analytics
spec:
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      # nodeSelector:
      #   kubernetes.io/hostname: k8s-haosu-18.sdsc.optiputer.net
      restartPolicy: Never
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      volumes:
        - name: workspace
          emptyDir: {}
        - name: wandb-cache
          emptyDir: {}
        - name: cb-results
          persistentVolumeClaim:
            claimName: cb-daily-checkpoints
        - name: weatherbench
          persistentVolumeClaim:
            claimName: weatherbench
        # GCS credentials (optional - only needed if bucket requires authentication)
        # Uncomment and create the secret if needed:
        # - name: gcs-credentials
        #   secret:
        #     secretName: gcs-credentials
      containers:
        - name: climatebench-edm
          image: gitlab-registry.nrp-nautilus.io/salvarc/climate-ml:latest
          imagePullPolicy: IfNotPresent
          workingDir: /workspace/climatebench
          env:
            - name: WANDB_API_KEY
              valueFrom:
                secretKeyRef:
                  name: ${WANDB_SECRET_NAME}
                  key: api-key
            - name: RUN_NAME
              value: ${JOB_NAME}
            - name: RUN_ID
              value: ${JOB_NAME}
            - name: DATA_DIR
              value: gs://weatherbench2/datasets/era5/1959-2022-6h-240x121_equiangular_with_poles_conservative.zarr
            - name: STATS_DIR
              value: /workspace/era5_statistics
            - name: WANDB_DIR
              value: /wandb
            - name: WANDB_CONFIG_DIR
              value: /wandb
            - name: WANDB_CACHE_DIR
              value: /wandb
            - name: WANDB_DATA_DIR
              value: /wandb
            - name: HYDRA_FULL_ERROR
              value: "1"
            - name: TORCH_CUDNN_V8_API_ENABLED
              value: "1"
            # GCS credentials (optional - only set if secret exists)
            # - name: GOOGLE_APPLICATION_CREDENTIALS
            #   value: /etc/gcs/key.json
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail
              mkdir -p /workspace
              cd /workspace
              
              # Use pre-cloned repository from persistent volume (weatherbench PVC) if available
              PRE_CLONED_REPO="/weatherbench/climatebench-repo"
              if [ -d "$PRE_CLONED_REPO" ] && [ -f "$PRE_CLONED_REPO/.git/config" ]; then
                echo "Using pre-cloned repository from: $PRE_CLONED_REPO"
                rm -rf climatebench
                cp -r "$PRE_CLONED_REPO" climatebench
                cd climatebench
                
                # Verify we're on the correct branch
                echo "Repository copied. Current branch:"
                git branch
                echo "Latest commit:"
                git log -1 --oneline
                
                # Ensure we're on main branch (default)
                CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
                if [ "$CURRENT_BRANCH" != "main" ]; then
                  echo "Switching to main branch..."
                  git checkout main || {
                    echo "WARNING: Could not checkout main branch, staying on $CURRENT_BRANCH"
                  }
                fi
              else
                echo "Pre-cloned repository not found. Cloning public repository..."
                rm -rf climatebench
                git clone https://github.com/pratham-aggr/diffusion-aimip.git climatebench
                cd climatebench
                # Repository uses main branch by default
              fi
              # Patch compute_era5_statistics.py: replace any os.path.exists check with GCS-aware version
              python -c "c=open('compute_era5_statistics.py').read(); import re; c=re.sub(r'os\.path\.exists\(zarr_path\)', 'zarr_path.startswith(\"gs://\") or os.path.exists(zarr_path)', c); c=re.sub(r'if\s+not\s+(zarr_path\.startswith|os\.path\.exists)', r'if not zarr_path.startswith(\"gs://\") and not os.path.exists', c); open('compute_era5_statistics.py','w').write(c); print('Patched os.path.exists checks')" || python -c "c=open('compute_era5_statistics.py').read(); c=c.replace('if not os.path.exists(zarr_path):', 'if not zarr_path.startswith(\"gs://\") and not os.path.exists(zarr_path):'); open('compute_era5_statistics.py','w').write(c); print('Patched')" || true
              # Patch wandb error handling: fix wandb.errors.errors.Error to wandb.errors.Error
              python -c "c=open('src/experiment_types/_base_experiment.py').read(); c=c.replace('except wandb.errors.errors.Error as e:', 'except wandb.errors.Error as e:'); open('src/experiment_types/_base_experiment.py','w').write(c); print('Patched wandb.errors.errors.Error')" || true
              python -m pip install --upgrade pip
              if [ -f requirements.txt ]; then
                pip install --no-cache-dir -r requirements.txt
              fi
              pip install --no-cache-dir cartopy boto3 xbatcher "dask[distributed]" gcsfs seaborn
              pip install --no-cache-dir -e .
              ERA5_STATS_DIR="${STATS_DIR:-/workspace/era5_statistics}"
              mkdir -p "$ERA5_STATS_DIR"
              # Verify GCS access if using GCS path
              if [[ "${DATA_DIR}" == gs://* ]]; then
                echo "Using GCS dataset: ${DATA_DIR}"
                echo "Checking GCS dataset access..."
                # Try with credentials if available, otherwise use default/anonymous
                if [ -f "/etc/gcs/key.json" ]; then
                  export GOOGLE_APPLICATION_CREDENTIALS=/etc/gcs/key.json
                  echo "Using GCS credentials from secret"
                else
                  echo "No GCS credentials found, using default authentication (public bucket or application-default)"
                fi
                python -c "import gcsfs; fs = gcsfs.GCSFileSystem(); print('GCS accessible:', fs.exists('${DATA_DIR}'))" || echo "GCS check failed, but continuing..."
              fi
              # Try to find the zarr dataset if the configured path doesn't exist (for local paths only)
              if [[ "${DATA_DIR}" != gs://* ]] && [ ! -d "${DATA_DIR}" ]; then
                echo "WARNING: Data directory ${DATA_DIR} not found. Searching for zarr dataset..."
                # Search for exact name first (check weatherbench PVC)
                ZARR_FILE=$(find /weatherbench -maxdepth 6 -name "1959-2022-6h-240x121_equiangular_with_poles_conservative.zarr" -type d 2>/dev/null | head -1)
                # If not found, search for similar patterns
                if [ -z "$ZARR_FILE" ]; then
                  echo "Searching for similar zarr datasets..."
                  ZARR_FILE=$(find /weatherbench -maxdepth 6 -name "*1959-2022*6h*240x121*.zarr" -type d 2>/dev/null | head -1)
                fi
                # If still not found, search for any ERA5 zarr
                if [ -z "$ZARR_FILE" ]; then
                  echo "Searching for any ERA5 zarr dataset..."
                  ZARR_FILE=$(find /weatherbench -maxdepth 6 -path "*/ERA5/*.zarr" -type d 2>/dev/null | head -1)
                fi
                if [ -n "$ZARR_FILE" ]; then
                  echo "Found zarr dataset at: $ZARR_FILE"
                  export DATA_DIR="$ZARR_FILE"
                else
                  echo "ERROR: Could not find zarr dataset. Searched in /weatherbench (maxdepth 6)."
                  echo "NOTE: Using GCS path is recommended: gs://weatherbench2/datasets/era5/..."
                  echo "Contents of /weatherbench:"
                  ls -la /weatherbench 2>/dev/null | head -20 || true
                  echo "Searching for any .zarr directories:"
                  find /weatherbench -maxdepth 4 -name "*.zarr" -type d 2>/dev/null | head -10 || true
                  exit 1
                fi
              else
                echo "Using configured data directory: ${DATA_DIR}"
              fi
              # Setup results directory with proper permissions first
              RESULTS_DIR_BASE=/results
              mkdir -p "$RESULTS_DIR_BASE" || true
              chmod -R 777 "$RESULTS_DIR_BASE" || true
              # Check persistent location first, then workspace
              # Use weatherbench PVC for persistent storage
              PERSISTENT_STATS_DIR=/weatherbench/era5_statistics
              mkdir -p "$PERSISTENT_STATS_DIR" || true
              chmod -R 777 "$PERSISTENT_STATS_DIR" || true
              if [ -f "$PERSISTENT_STATS_DIR/era5_mean.nc" ] && [ -f "$PERSISTENT_STATS_DIR/era5_std.nc" ]; then
                echo "Found existing statistics in persistent location: $PERSISTENT_STATS_DIR"
                mkdir -p "$ERA5_STATS_DIR"
                cp "$PERSISTENT_STATS_DIR/era5_mean.nc" "$ERA5_STATS_DIR/" || true
                cp "$PERSISTENT_STATS_DIR/era5_std.nc" "$ERA5_STATS_DIR/" || true
                echo "Copied statistics to $ERA5_STATS_DIR"
              elif [ ! -f "$ERA5_STATS_DIR/era5_mean.nc" ] || [ ! -f "$ERA5_STATS_DIR/era5_std.nc" ]; then
                echo "Computing statistics (this may take 30-60 minutes)..."
                python compute_era5_statistics.py \
                  --zarr-path ${DATA_DIR} \
                  --output-dir "$ERA5_STATS_DIR" \
                  --train-start 1979-01-01 \
                  --train-end 2020-12-31 \
                  --force
                # Copy computed stats to persistent location for future runs
                mkdir -p "$PERSISTENT_STATS_DIR" || true
                chmod -R 777 "$PERSISTENT_STATS_DIR" || true
                cp "$ERA5_STATS_DIR/era5_mean.nc" "$PERSISTENT_STATS_DIR/" 2>/dev/null || true
                cp "$ERA5_STATS_DIR/era5_std.nc" "$PERSISTENT_STATS_DIR/" 2>/dev/null || true
                echo "Saved statistics to persistent location: $PERSISTENT_STATS_DIR"
              else
                echo "Statistics already exist in $ERA5_STATS_DIR"
              fi
              RESULTS_DIR=$RESULTS_DIR_BASE/${RUN_ID}
              if ! mkdir -p "$RESULTS_DIR/logs"; then
                echo "WARNING: Unable to write to $RESULTS_DIR_BASE. Falling back to /workspace/results."
                RESULTS_DIR_BASE=/workspace/results
                RESULTS_DIR=$RESULTS_DIR_BASE/${RUN_ID}
                mkdir -p "$RESULTS_DIR/logs"
              fi
              python run.py \
                experiment=era5_sst_seaice_emulation_edm_aimip_tuned_v2 \
                trainer.accelerator="gpu" \
                trainer.devices=2 \
                work_dir=$RESULTS_DIR \
                ckpt_dir=$RESULTS_DIR/checkpoints \
                log_dir=$RESULTS_DIR/logs \
                datamodule.data_dir=${DATA_DIR} \
                datamodule.batch_size=64 \
                datamodule.batch_size_per_gpu=16 \
                +datamodule.data_dir_stats=${STATS_DIR} \
                +module.pr_clamping=false \
                logger.wandb.project="ERA5-Climate-Emulation" \
                logger.wandb.entity="climate-analytics-lab" \
                logger.wandb.save_to_wandb=true \
                logger.wandb.save_to_s3_bucket=false \
                ++logger.wandb.mode="online"
          resources:
            requests:
              nvidia.com/gpu: 2
              cpu: "8"
              memory: "40Gi"
            limits:
              nvidia.com/gpu: 2
              cpu: "9.6"
              memory: "48Gi"
          volumeMounts:
            - name: workspace
              mountPath: /workspace
            - name: wandb-cache
              mountPath: /wandb
            - name: cb-results
              mountPath: /results
            - name: weatherbench
              mountPath: /weatherbench
            # GCS credentials mount (uncomment if secret exists)
            # - name: gcs-credentials
            #   mountPath: /etc/gcs
            #   readOnly: true

