import os.path
import signal
import time

import hydra
import pytorch_lightning as pl
import torch
from omegaconf import DictConfig

import src.utilities.config_utils as cfg_utils
import wandb
from src.interface import get_model_and_data
from src.utilities.utils import AlreadyLoggedError, divein, get_logger, melk
from src.utilities.wandb_api import get_run_api
from src.utilities.wandb_callbacks import MyWandbLogger


log = get_logger(__name__)


def run_model(config: DictConfig) -> float:
    r"""
    This function runs/trains/tests the model.

    .. note::
        It is recommended to call this function by running its underlying script, ``src.train.py``,
        as this will enable you to make the best use of the command line integration with Hydra.
        For example, you can easily train a UNet for 10 epochs on the CPU with:

        >>>  python train.py trainer.max_epochs=10 trainer.accelerator="cpu" model=unet_resnet callbacks=default

    Args:
        config: A DictConfig object generated by hydra containing the model, data, callbacks & trainer configuration.

    Returns:
        float: the best model score reached while training the model.
                E.g. "val/mse", the mean squared error on the validation set.
    """
    # Seed for reproducibility
    pl.seed_everything(config.seed)
    ckpt_path_orig = config.get("ckpt_path")

    # If not resuming training, check if run already exists (with same hyperparameters and seed)
    config = cfg_utils.extras(config, if_wandb_run_already_exists="resume")

    # Init Lightning callbacks and loggers (e.g. model checkpointing and Wandb logger)
    callbacks = cfg_utils.get_all_instantiable_hydra_modules(config, "callbacks")
    loggers = cfg_utils.get_all_instantiable_hydra_modules(config, "logger")

    wandb_id = config.logger.wandb.get("id") if config.get("logger") and config.logger.get("wandb") else None
    uses_wandb = wandb_id is not None
    # Get wandb.loggers.WandbLogger instance
    if uses_wandb:
        wandb_logger = [logger for logger in loggers if isinstance(logger, MyWandbLogger)]
        assert len(wandb_logger) == 1, f"Expected exactly one MyWandbLogger, but got {len(wandb_logger)}!"
        wandb_logger = wandb_logger[0]
        cfg_utils.save_hydra_config_to_wandb(config)

    # Print config. For pretty print, rich package needs to be installed (optional)
    if config.get("print_config"):
        cfg_utils.print_config(config, fields="all")

    if uses_wandb and config.wandb_status == "resume":
        # Reload model checkpoint if needed to resume training
        # Set the checkpoint to reload
        ckpt_filename = config.get("ckpt_path")
        if config.get("eval_mode"):
            # Use best model checkpoint for testing
            ckpt_filename = (
                ckpt_filename
                or os.path.join(config.ckpt_dir.replace("-test", ""), config.logger.wandb.training_id, ckpt_path_orig)
                or "best.ckpt"
            )
        else:
            # Use last model checkpoint for resuming training
            if ckpt_filename is None:
                ckpt_filename = "last.ckpt"
            elif not ckpt_filename.endswith("last.ckpt"):
                log.warning(f'Checkpoint used to resume training is not "last.ckpt" but "{ckpt_filename}"!')

        if os.path.exists(ckpt_filename) and str(config.logger.wandb.training_id) in str(ckpt_filename):
            # Load model checkpoint from local file. For safety, only do this if the wandb run id is in the path.
            ckpt_path = ckpt_filename
            log.info(f"Loading model weights from local checkpoint: ``{ckpt_path}``")
        else:
            # if config.get("eval_mode") and os.path.exists(ckpt_filename):
            # print(f"ckpt_filename: {ckpt_filename} exists. ckpt_path=", config.get("ckpt_path"))
            # Note: train_run_path can be different from wandb.run.path
            train_run_path = config.logger.wandb.get("train_run_path", config.logger.wandb.run_path)
            training_run_id = config.logger.wandb.training_id
            assert (
                str(training_run_id) in train_run_path
            ), f"Training run id {training_run_id} not in run path {train_run_path}"
            ckpt_path = f"{training_run_id}-{ckpt_filename}"
            if os.path.exists(ckpt_path):
                try:
                    os.remove(ckpt_path)  # Re-download the model checkpoint in case it is obsolete version
                except FileNotFoundError:
                    pass  # weird but ok
            assert not os.path.exists(ckpt_path), f"{ckpt_path=} already exists!"
            if True:  # not os.path.exists(ckpt_path):
                # Download model checkpoint from wandb (using the training run id)
                restored_path = wandb_logger.restore_checkpoint(
                    ckpt_filename, ckpt_path, run_path=train_run_path, root=os.getcwd(), restore_from="any"
                )
                if restored_path is None:
                    # No checkpoint found - start from scratch
                    log.warning(f"No checkpoint found for run {training_run_id}. Starting training from scratch.")
                    ckpt_path = None
                else:
                    ckpt_path = restored_path
    else:
        if config.get("eval_mode"):
            ckpt_path = config.get("ckpt_path")
            assert os.path.isfile(ckpt_path), f"Checkpoint file not found: {ckpt_path} but {config.eval_mode=}"
        else:
            ckpt_path = None

    # Obtain the instantiated model and data classes from the config
    model, datamodule = get_model_and_data(config)

    # Init Lightning trainer
    trainer: pl.Trainer = hydra.utils.instantiate(config.trainer, callbacks=callbacks, logger=loggers)

    # Send some parameters from config to be saved by the lightning loggers
    cfg_utils.log_hyperparameters(
        config=config, model=model, data_module=datamodule, trainer=trainer, callbacks=callbacks
    )
    if config.get("eval_mode"):
        if config.get("model") is not None:
            if config.get("force_no_ckpt"):
                ckpt_path = ckpt_path_orig = None
                ckpt = {}
                log.warning("Forcing ignoring the checkpoint path. This will use random init. weights!")
            else:
                assert ckpt_path is not None, "ckpt_path must be provided in eval_mode"
                # Log epoch and step of the model checkpoint to be tested
                ckpt = torch.load(ckpt_path, weights_only=False, map_location="cpu")
            epoch, global_step = ckpt.get("epoch", -1), ckpt.get("global_step", -1)
            model._default_global_step = global_step
            model._default_epoch = epoch
            if trainer.logger is not None:
                step_of_ckpt = {
                    "epoch": epoch,
                    "global_step": global_step,
                    "ckpt_path_orig": ckpt_path_orig,
                    "ckpt_script_path": ckpt.get("script_path", None),
                }
                trainer.logger.log_hyperparams(step_of_ckpt)

    else:
        if hasattr(signal, "SIGUSR1"):  # Windows does not support signals
            signal.signal(signal.SIGUSR1, melk(trainer, config.ckpt_dir))
            signal.signal(signal.SIGUSR2, divein(trainer))

        try:
            # Train the model
            trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
            log.info(" ---------------- Training finished successfully ----------------")
        except Exception as e:
            melk(trainer, config.ckpt_dir)()
            raise e

    # Testing:
    if config.get("test_after_training") or config.get("eval_mode") == "test":
        if config.get("eval_mode") == "test":
            test_what = {"ckpt_path": ckpt_path, "model": model}
        else:
            # Testing after training --> use the best model checkpoint
            test_what = {"ckpt_path": "best"} if hasattr(callbacks, "model_checkpoint") else {"model": model}

        try:
            trainer.test(datamodule=datamodule, **test_what)
        except AlreadyLoggedError as e:
            log.warning(f"Test already logged: {e}")
        if uses_wandb and config.logger.wandb.get("train_run_path") is not None:
            # Set flag to indicate that the model has been tested
            train_run_api = get_run_api(run_path=config.logger.wandb.train_run_path)
            # train_run_api.summary.get("epoch", -1)
            train_run_api.summary["tested"] = True
            train_run_api.update()

    if config.get("eval_mode") == "validate":
        # Validate using the model
        if hasattr(model.hparams, "inference_val_every_n_epochs"):
            model.hparams.inference_val_every_n_epochs = 1
        trainer.validate(model=model, datamodule=datamodule, ckpt_path=ckpt_path)

    if config.get("eval_mode") == "predict":
        # Predict using the model
        trainer.predict(model=model, datamodule=datamodule, ckpt_path=ckpt_path)

    if uses_wandb:
        try:
            time.sleep(5)  # Sleep for 5 seconds to not over-print the wandb finish message
            wandb.finish()
            log.info(" ---------------- Sleeping for 5 seconds to make sure wandb finishes... ----------------")
            time.sleep(5)
        except (FileNotFoundError, PermissionError) as e:
            log.info(f"Wandb finish error:\n{e}")

    if trainer.checkpoint_callback and trainer.checkpoint_callback.best_model_path and False:
        # This is how the best model weights can be reloaded back:
        model.load_from_checkpoint(
            trainer.checkpoint_callback.best_model_path,
            datamodule_config=config.datamodule,
        )

    # return best score (e.g. validation mse). This is useful when using Hydra+Optuna HP tuning.
    return trainer.checkpoint_callback.best_model_score


@hydra.main(config_path="configs/", config_name="main_config.yaml", version_base=None)
def main(config: DictConfig) -> float:
    """Run/train model based on the config file configs/main_config.yaml (and any command-line overrides)."""
    return run_model(config)


if __name__ == "__main__":
    main()
