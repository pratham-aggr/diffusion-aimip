# @package _global_

# AIMIP-1 SUBMISSION - EDM (Elucidated Diffusion Model) for Climate Emulation
# Configuration for AIMIP Phase 1 submission requirements
# 
# Key differences from standard config:
# - Forcing: SST + Sea-ice (instead of SST + Solar)
# - Extended outputs: All AIMIP-required fields
# - Test period: 2015-2024 (10 years, not 6)
#
# Run with: python run.py experiment=era5_sst_seaice_emulation_edm_aimip

defaults:
  - override /datamodule: era5_sst_seaice_aimip.yaml
  - override /model: adm.yaml  # Same UNet architecture
  - override /module: emulation.yaml  # Emulation module for inputâ†’output mapping
  - override /diffusion: edm.yaml  # EDM diffusion process
  - override /trainer: ddp.yaml  # Use DDP for multi-GPU training
  - override /callbacks: comprehensive_wandb.yaml  # Use comprehensive logging
  - _self_

name: "EmulationSST-SeaIce-EDM-AIMIP"

tags:
  - "climate_emulation"
  - "sst_forcing"
  - "sea_ice_forcing"
  - "diffusion_model"
  - "edm"
  - "aimip"
  - "aimip_submission"

trainer:
  max_epochs: 40  # Same as fast_local config
  devices: 2  # Use 2 GPUs (can increase if needed)
  accelerator: "gpu"
  strategy: "ddp_find_unused_parameters_false"  # DDP strategy for multi-GPU
  precision: "16-mixed"  # Mixed precision for faster training
  gradient_clip_val: 5.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  num_sanity_val_steps: 0  # Disable sanity check to avoid DDP deadlock
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  benchmark: True
  deterministic: False

datamodule:
  hourly_resolution: 6  # 6-hourly data
  window: 1
  horizon: 1
  batch_size: 4  # May need to reduce further with more output variables
  eval_batch_size: 2
  num_workers: 2  # Reduced to avoid DDP I/O issues
  pin_memory: True
  prefetch_factor: 4
  persistent_workers: True
  loss_pressure_weighting_levels: [1000, 850, 700, 500, 250, 100, 50]  # Enable for 3D fields

model:
  model_channels: 256  # Same capacity
  channel_mult: [1, 2, 3, 4]
  num_blocks: 3
  attn_resolutions: [32, 16, 8]
  dropout: 0.1
  upsample_dims: [256, 128]
  upsample_outputs_by: 1
  outer_sample_mode: "bilinear"
  with_time_emb: True  # REQUIRED for EDM - time embeddings for noise level

diffusion:
  # Loss function
  loss_function: "wmse"  # Use weighted MSE loss
  
  # EDM-specific parameters
  sigma_min: 0.002
  sigma_max_inf: 80.0
  P_mean: -1.2
  P_std: 1.2
  noise_distribution: "lognormal"
  
  # Sampling parameters
  num_steps: 18
  rho: 7
  S_churn: 0.05
  S_min: 0.0
  S_max: .inf
  S_noise: 1.0
  heun: True
  
  # Logging
  compute_loss_per_sigma: False  # Disabled to reduce memory usage

module:
  stack_window_to_channel_dim: False
  enable_inference_dropout: False
  monitor: "val/avg/rmse"
  use_ema: True
  ema_decay: 0.9999
  num_predictions: 1
  
  # Optimizer
  optimizer:
    name: "AdamW"
    lr: 1e-4  # Higher LR for faster convergence
    weight_decay: 0.01
    eps: 1e-08
    betas: [0.9, 0.95]
  
  # Cosine annealing with warmup
  scheduler:
    name: "linear_warmup_cosine"
    warmup_epochs: 5
    max_epochs: ${trainer.max_epochs}
    warmup_start_lr: 1e-6
    eta_min: 1e-6
    interval: "step"

callbacks:
  model_checkpoint:
    monitor: "val/avg/rmse"
    mode: "min"
    save_top_k: 3
    save_last: True
    verbose: True
  
  early_stopping:
    monitor: "val/avg/rmse"
    patience: 12
    min_delta: 0.001
    mode: "min"
    verbose: True
  
  learning_rate_logging:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "step"

logger:
  wandb:
    project: "ERA5-Climate-Emulation"
    name: "EmulationSST-SeaIce-EDM-AIMIP"
    tags: ${tags}
    group: "sst_seaice_emulation_aimip"
    job_type: "diffusion"
    mode: "online"
    save_to_wandb: true
    save_to_s3_bucket: false
    notes: |
      AIMIP-1 SUBMISSION - EDM Diffusion Model for Climate Emulation
      - Inputs: SST + Sea-ice concentration (AMIP protocol)
      - Outputs: All AIMIP-required fields (3D + surface)
      - Training: 1979-2014
      - Test: 2015-2024 (10 years)
      - Model: DhariwalUNet with time embeddings
      - Denoising steps: 18
      - Learning rate: 1e-4 with 5-epoch warmup
